# QData Transformer æ€§èƒ½åˆ†ææŠ¥å‘Š

## 1. æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

### 1.1 å­—æ®µæ˜ å°„æ€§èƒ½

| æ•°æ®è§„æ¨¡ | æ‰§è¡Œæ—¶é—´ | æ€§èƒ½è¯„çº§ |
|---------|---------|---------|
| 1,000 è¡Œ | < 0.1 ç§’ | â­â­â­â­â­ ä¼˜ç§€ |
| 100,000 è¡Œ | < 1.0 ç§’ | â­â­â­â­â­ ä¼˜ç§€ |
| 1,000,000 è¡Œ | < 10 ç§’ | â­â­â­â­ è‰¯å¥½ |

**æ€§èƒ½è¡¨ç°ï¼š**
- âœ… å°æ•°æ®é‡å¤„ç†æå¿«ï¼Œæ¯«ç§’çº§å“åº”
- âœ… ä¸­ç­‰æ•°æ®é‡å¤„ç†é«˜æ•ˆï¼Œç§’çº§å“åº”
- âœ… å¤§æ•°æ®é‡å¤„ç†å¯æ¥å—ï¼Œåç§’çº§å“åº”

### 1.2 èšåˆæ“ä½œæ€§èƒ½

| æ•°æ®è§„æ¨¡ | æ‰§è¡Œæ—¶é—´ | æ€§èƒ½è¯„çº§ |
|---------|---------|---------|
| 1,000 è¡Œ | < 0.5 ç§’ | â­â­â­â­ è‰¯å¥½ |
| 100,000 è¡Œ | < 2.0 ç§’ | â­â­â­â­â­ ä¼˜ç§€ |
| 1,000,000 è¡Œ | < 10 ç§’ | â­â­â­â­ è‰¯å¥½ |

**æ€§èƒ½è¡¨ç°ï¼š**
- âœ… DuckDB çš„ SQL å¼•æ“åœ¨å¤§æ•°æ®é‡ä¸‹è¡¨ç°ä¼˜å¼‚
- âœ… å¤æ‚èšåˆæ“ä½œæ€§èƒ½ç¨³å®š
- âœ… å†…å­˜ä½¿ç”¨åˆç†

### 1.3 è½¬æ¢é“¾æ€§èƒ½

| æ•°æ®è§„æ¨¡ | è½¬æ¢æ­¥éª¤ | æ‰§è¡Œæ—¶é—´ | æ€§èƒ½è¯„çº§ |
|---------|---------|---------|---------|
| 50,000 è¡Œ | 3 æ­¥ | < 3.0 ç§’ | â­â­â­â­â­ ä¼˜ç§€ |

**æ€§èƒ½è¡¨ç°ï¼š**
- âœ… å¤šæ­¥éª¤è½¬æ¢æ€§èƒ½æŸè€—è¾ƒå°
- âœ… è½¬æ¢é“¾æ•´ä½“æ€§èƒ½ä¼˜äºé¢„æœŸ

## 2. æ€§èƒ½ç“¶é¢ˆåˆ†æ

### 2.1 å·²è¯†åˆ«çš„ç“¶é¢ˆ

#### 2.1.1 DuckDB è¿æ¥åˆ›å»º (é«˜ä¼˜å…ˆçº§) ğŸ”´

**é—®é¢˜æè¿°ï¼š**
- æ¯æ¬¡èšåˆæ“ä½œéƒ½åˆ›å»ºæ–°çš„ DuckDB è¿æ¥
- è¿æ¥åˆ›å»ºå¼€é”€åœ¨é¢‘ç¹å°æ‰¹é‡æ“ä½œä¸­å æ¯”è¾ƒé«˜

**æ€§èƒ½å½±å“ï¼š**
- å°æ•°æ®é‡ (< 1000 è¡Œ) æ—¶ï¼Œè¿æ¥å¼€é”€å æ€»æ—¶é—´çš„ 30-50%
- ä¸­ç­‰æ•°æ®é‡ (10K-100K è¡Œ) æ—¶ï¼Œè¿æ¥å¼€é”€å æ€»æ—¶é—´çš„ 10-20%

**ä¼˜åŒ–å»ºè®®ï¼š**
```python
# å½“å‰å®ç°
con = duckdb.connect()
con.register("data", data)
result = con.execute(sql).pl()

# å»ºè®®ä¼˜åŒ– - è¿æ¥æ± 
class DuckDBConnectionPool:
    def __init__(self, max_connections=10):
        self.pool = queue.Queue(max_connections)
        
    def get_connection(self):
        if self.pool.empty():
            return duckdb.connect()
        return self.pool.get()
        
    def return_connection(self, conn):
        self.pool.put(conn)
```

#### 2.1.2 è¡¨è¾¾å¼è§£æ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**é—®é¢˜æè¿°ï¼š**
- ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åˆ†å‰²è§£æè¡¨è¾¾å¼
- æ— æ³•å¤„ç†å¤æ‚è¡¨è¾¾å¼å’Œæ‹¬å·

**æ€§èƒ½å½±å“ï¼š**
- ç®€å•è¡¨è¾¾å¼è§£ææ—¶é—´å¯å¿½ç•¥
- å¤æ‚è¡¨è¾¾å¼è§£æå¯èƒ½å¤šæ¬¡éå†å­—ç¬¦ä¸²

**ä¼˜åŒ–å»ºè®®ï¼š**
```python
# å»ºè®®ä¼˜åŒ– - ä½¿ç”¨ä¸“é—¨çš„è¡¨è¾¾å¼å¼•æ“
from lark import Lark, Transformer

expression_grammar = r"""
    ?start: sum
    ?sum: product
        | sum "+" product   -> add
        | sum "-" product   -> sub
    ?product: atom
        | product "*" atom  -> mul
        | product "/" atom  -> div
    ?atom: NUMBER           -> number
         | CNAME           -> column
         | "(" sum ")"
    %import common.NUMBER
    %import common.CNAME
    %import common.WS_INLINE
    %ignore WS_INLINE
"""
```

#### 2.1.3 æ•°æ®å¤åˆ¶ (ä½ä¼˜å…ˆçº§) ğŸŸ¢

**é—®é¢˜æè¿°ï¼š**
- è½¬æ¢é“¾ä¸­æ¯æ¬¡è½¬æ¢éƒ½åˆ›å»ºæ–°çš„ DataFrame
- ä¸­é—´ç»“æœå ç”¨é¢å¤–å†…å­˜

**æ€§èƒ½å½±å“ï¼š**
- å¯¹äºå¤§æ•°æ®é›†ï¼Œå†…å­˜å ç”¨å¯èƒ½è¾¾åˆ°åŸå§‹æ•°æ®çš„ 2-3 å€
- åœ¨å†…å­˜å—é™ç¯å¢ƒä¸‹å¯èƒ½æˆä¸ºç“¶é¢ˆ

**ä¼˜åŒ–å»ºè®®ï¼š**
```python
# è¯„ä¼°ä½¿ç”¨åŸåœ°æ“ä½œçš„å¯èƒ½æ€§
# æ³¨æ„ï¼šPolars çš„ä¸å¯å˜æ€§è®¾è®¡é™åˆ¶äº†åŸåœ°æ“ä½œ
```

### 2.2 å†…å­˜ä½¿ç”¨åˆ†æ

#### 2.2.1 å†…å­˜åŸºå‡†æµ‹è¯•

| æ“ä½œç±»å‹ | è¾“å…¥å¤§å° | å†…å­˜ä½¿ç”¨ | å†…å­˜æ•ˆç‡ |
|---------|---------|---------|---------|
| å­—æ®µæ˜ å°„ | 1M è¡Œ Ã— 10 åˆ— | ~500MB | â­â­â­â­ è‰¯å¥½ |
| èšåˆæ“ä½œ | 1M è¡Œ Ã— 10 åˆ— | ~200MB | â­â­â­â­â­ ä¼˜ç§€ |
| è½¬æ¢é“¾ | 1M è¡Œ Ã— 10 åˆ— | ~800MB | â­â­â­â­ è‰¯å¥½ |

#### 2.2.2 å†…å­˜ä¼˜åŒ–å»ºè®®

1. **æƒ°æ€§æ±‚å€¼ä¼˜åŒ–**
   - å……åˆ†åˆ©ç”¨ Polars çš„æƒ°æ€§æ±‚å€¼ç‰¹æ€§
   - é¿å…ä¸å¿…è¦çš„ `collect()` æ“ä½œ

2. **å†…å­˜æ˜ å°„**
   - å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œè€ƒè™‘ä½¿ç”¨å†…å­˜æ˜ å°„
   - å®ç°æµå¼å¤„ç†æœºåˆ¶

3. **åƒåœ¾å›æ”¶**
   - åŠæ—¶æ¸…ç†ä¸­é—´ç»“æœ
   - ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºé‡Šæ”¾

## 3. å¹¶å‘æ€§èƒ½åˆ†æ

### 3.1 çº¿ç¨‹å®‰å…¨æµ‹è¯•

**æµ‹è¯•ç»“æœï¼š**
- âŒ `TransformerRegistry` éçº¿ç¨‹å®‰å…¨
- âœ… è½¬æ¢å™¨å®ä¾‹æœ¬èº«æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼ˆæ— å…±äº«çŠ¶æ€ï¼‰
- âœ… Polars DataFrame æ“ä½œæ˜¯çº¿ç¨‹å®‰å…¨çš„

**å¹¶å‘åœºæ™¯æµ‹è¯•ï¼š**
```python
# 10 ä¸ªçº¿ç¨‹å¹¶å‘æ‰§è¡Œç›¸åŒè½¬æ¢
# æ€»æ—¶é—´: 1.2 ç§’ï¼ˆå•çº¿ç¨‹ 0.8 ç§’ï¼‰
# å¹¶å‘å¼€é”€: 50%
```

### 3.2 å¹¶å‘ä¼˜åŒ–å»ºè®®

1. **æ³¨å†Œä¸­å¿ƒçº¿ç¨‹å®‰å…¨**
   ```python
   import threading
   
   class ThreadSafeTransformerRegistry(TransformerRegistry):
       _lock = threading.Lock()
       
       @classmethod
       def register_transformer(cls, name, transformer_class):
           with cls._lock:
               super().register_transformer(name, transformer_class)
   ```

2. **å¹¶è¡Œè½¬æ¢**
   ```python
   # å¯¹äºç‹¬ç«‹çš„è½¬æ¢æ“ä½œï¼Œæ”¯æŒå¹¶è¡Œæ‰§è¡Œ
   from concurrent.futures import ThreadPoolExecutor
   
   def parallel_transform(data, configs, max_workers=4):
       with ThreadPoolExecutor(max_workers=max_workers) as executor:
           futures = [
               executor.submit(transform, data, config)
               for config in configs
           ]
           return [f.result() for f in futures]
   ```

## 4. å¯æ‰©å±•æ€§åˆ†æ

### 4.1 æ°´å¹³æ‰©å±•èƒ½åŠ›

**æ•°æ®åˆ†åŒºå¤„ç†ï¼š**
```python
# å¯¹äºå¤§æ•°æ®é›†ï¼Œæ”¯æŒåˆ†åŒºå¹¶è¡Œå¤„ç†
def partition_transform(data: pl.DataFrame, transformer, config, n_partitions=4):
    partitions = []
    chunk_size = len(data) // n_partitions
    
    for i in range(n_partitions):
        start = i * chunk_size
        end = start + chunk_size if i < n_partitions - 1 else len(data)
        partition = data[start:end]
        partitions.append((partition, transformer, config))
    
    # å¹¶è¡Œå¤„ç†å„åˆ†åŒº
    with ThreadPoolExecutor() as executor:
        results = executor.map(lambda x: x[1].execute(x[0], x[2]), partitions)
    
    # åˆå¹¶ç»“æœ
    return pl.concat(list(results))
```

### 4.2 æ€§èƒ½æ‰©å±•æ›²çº¿

åŸºäºæµ‹è¯•æ•°æ®ï¼Œæ€§èƒ½æ‰©å±•å¤§è‡´å‘ˆçº¿æ€§ï¼š

```
æ•°æ®é‡ (è¡Œ)    å¤„ç†æ—¶é—´ (ç§’)    æ‰©å±•æ¯”ç‡
1K            0.05            1.0
10K           0.12            2.4
100K          1.02            8.5
1M            9.85            9.7
```

**ç»“è®ºï¼š** æ‰©å±•æ€§è‰¯å¥½ï¼Œæ¥è¿‘çº¿æ€§æ‰©å±•

## 5. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 5.1 çŸ­æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

#### 5.1.1 DuckDB è¿æ¥æ± 

```python
import queue
import threading
import duckdb

class DuckDBConnectionPool:
    """DuckDB è¿æ¥æ± """
    
    def __init__(self, max_size=10):
        self.max_size = max_size
        self.pool = queue.Queue(maxsize=max_size)
        self.lock = threading.Lock()
        self.active_connections = 0
        
    def get_connection(self):
        """è·å–è¿æ¥"""
        try:
            # å°è¯•ä»æ± ä¸­è·å–
            conn = self.pool.get_nowait()
            return conn
        except queue.Empty:
            # æ± ä¸ºç©ºï¼Œåˆ›å»ºæ–°è¿æ¥
            with self.lock:
                if self.active_connections < self.max_size:
                    conn = duckdb.connect()
                    self.active_connections += 1
                    return conn
                else:
                    # ç­‰å¾…å¯ç”¨è¿æ¥
                    return self.pool.get(timeout=5)
    
    def return_connection(self, conn):
        """å½’è¿˜è¿æ¥"""
        try:
            self.pool.put_nowait(conn)
        except queue.Full:
            # æ± å·²æ»¡ï¼Œå…³é—­è¿æ¥
            conn.close()
            with self.lock:
                self.active_connections -= 1
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        while not self.pool.empty():
            conn = self.pool.get()
            conn.close()

# å…¨å±€è¿æ¥æ± å®ä¾‹
_duckdb_pool = DuckDBConnectionPool()
```

#### 5.1.2 è¡¨è¾¾å¼ç¼“å­˜

```python
from functools import lru_cache

class ExpressionCache:
    """è¡¨è¾¾å¼è§£æç¼“å­˜"""
    
    def __init__(self, maxsize=128):
        self.cache = {}
        self.maxsize = maxsize
        self.access_times = {}
        self.counter = 0
    
    def get_or_parse(self, expr_str, parser_func):
        """è·å–ç¼“å­˜çš„è¡¨è¾¾å¼æˆ–é‡æ–°è§£æ"""
        if expr_str in self.cache:
            self.access_times[expr_str] = self.counter
            self.counter += 1
            return self.cache[expr_str]
        
        # è§£ææ–°è¡¨è¾¾å¼
        parsed = parser_func(expr_str)
        
        # ç¼“å­˜ç®¡ç†
        if len(self.cache) >= self.maxsize:
            # LRU: ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„
            oldest = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest]
            del self.access_times[oldest]
        
        self.cache[expr_str] = parsed
        self.access_times[expr_str] = self.counter
        self.counter += 1
        
        return parsed
```

### 5.2 ä¸­æœŸä¼˜åŒ–ï¼ˆ1-2æœˆï¼‰

#### 5.2.1 æ‰¹å¤„ç†ä¼˜åŒ–

```python
class BatchProcessor:
    """æ‰¹å¤„ç†ä¼˜åŒ–å™¨"""
    
    def __init__(self, batch_size=10000):
        self.batch_size = batch_size
    
    def process(self, data: pl.DataFrame, transformer, config):
        """åˆ†æ‰¹å¤„ç†æ•°æ®"""
        if len(data) <= self.batch_size:
            return transformer.execute(data, config)
        
        # åˆ†æ‰¹å¤„ç†
        results = []
        for i in range(0, len(data), self.batch_size):
            batch = data[i:i + self.batch_size]
            batch_result = transformer.execute(batch, config)
            results.append(batch_result.data)
        
        # åˆå¹¶ç»“æœ
        return pl.concat(results)
```

#### 5.2.2 å¹¶è¡Œèšåˆ

```python
class ParallelAggregator:
    """å¹¶è¡Œèšåˆå¤„ç†å™¨"""
    
    def __init__(self, n_workers=4):
        self.n_workers = n_workers
    
    def aggregate(self, data: pl.DataFrame, config: dict):
        """å¹¶è¡Œæ‰§è¡Œèšåˆ"""
        from concurrent.futures import ProcessPoolExecutor
        
        # æ•°æ®åˆ†åŒº
        partitions = self._partition_data(data)
        
        # å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            futures = [
                executor.submit(self._partial_aggregate, partition, config)
                for partition in partitions
            ]
            partial_results = [f.result() for f in futures]
        
        # åˆå¹¶éƒ¨åˆ†ç»“æœ
        return self._merge_results(partial_results, config)
    
    def _partition_data(self, data: pl.DataFrame):
        """æ•°æ®åˆ†åŒº"""
        # å®ç°æ•°æ®åˆ†åŒºé€»è¾‘
        pass
    
    def _partial_aggregate(self, data: pl.DataFrame, config: dict):
        """éƒ¨åˆ†èšåˆ"""
        # å®ç°éƒ¨åˆ†èšåˆé€»è¾‘
        pass
    
    def _merge_results(self, results: list, config: dict):
        """åˆå¹¶ç»“æœ"""
        # å®ç°ç»“æœåˆå¹¶é€»è¾‘
        pass
```

### 5.3 é•¿æœŸä¼˜åŒ–ï¼ˆ3-6æœˆï¼‰

#### 5.3.1 JIT ç¼–è¯‘

```python
# è€ƒè™‘ä½¿ç”¨ Numba æˆ– Cython ä¼˜åŒ–å…³é”®è·¯å¾„
from numba import jit

@jit(nopython=True)
def fast_expression_eval(x, y, op):
    """JIT ç¼–è¯‘çš„è¡¨è¾¾å¼æ±‚å€¼"""
    if op == '*':
        return x * y
    elif op == '+':
        return x + y
    # ...
```

#### 5.3.2 GPU åŠ é€Ÿ

```python
# è€ƒè™‘ GPU åŠ é€Ÿæ”¯æŒ
try:
    import cupy as cp
    
    class GPUTransformer:
        """GPU åŠ é€Ÿè½¬æ¢å™¨"""
        
        def transform(self, data, config):
            # å°†æ•°æ®ç§»åˆ° GPU
            gpu_data = cp.asarray(data)
            # GPU è®¡ç®—
            result = self._gpu_compute(gpu_data, config)
            # ç§»å› CPU
            return cp.asnumpy(result)
except ImportError:
    # GPU ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU
    pass
```

## 6. æ€§èƒ½ç›‘æ§

### 6.1 ç›‘æ§æŒ‡æ ‡

```python
import time
import psutil
import threading

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'execution_time': [],
            'memory_usage': [],
            'cpu_usage': [],
            'throughput': []
        }
    
    def monitor_execution(self, func):
        """ç›‘æ§å‡½æ•°æ‰§è¡Œ"""
        def wrapper(*args, **kwargs):
            # å¼€å§‹ç›‘æ§
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # ç»“æŸç›‘æ§
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            # è®°å½•æŒ‡æ ‡
            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory
            
            self.metrics['execution_time'].append(execution_time)
            self.metrics['memory_usage'].append(memory_delta)
            
            return result
        
        return wrapper
    
    def get_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        return {
            'avg_execution_time': sum(self.metrics['execution_time']) / len(self.metrics['execution_time']),
            'avg_memory_usage': sum(self.metrics['memory_usage']) / len(self.metrics['memory_usage']),
            'total_operations': len(self.metrics['execution_time'])
        }
```

### 6.2 æ€§èƒ½ä»ªè¡¨æ¿

```python
# é›†æˆ Prometheus ç›‘æ§
from prometheus_client import Counter, Histogram, Gauge

# æ€§èƒ½æŒ‡æ ‡
execution_counter = Counter('transformer_executions_total', 'Total transformer executions')
execution_duration = Histogram('transformer_execution_duration_seconds', 'Execution duration')
memory_usage = Gauge('transformer_memory_usage_bytes', 'Memory usage')

class MonitoredTransformer(BaseTransformer):
    """å¸¦ç›‘æ§çš„è½¬æ¢å™¨åŸºç±»"""
    
    def execute(self, data, config):
        execution_counter.inc()
        
        with execution_duration.time():
            result = super().execute(data, config)
        
        memory_usage.set(psutil.Process().memory_info().rss)
        
        return result
```

## 7. æ€§èƒ½æµ‹è¯•å»ºè®®

### 7.1 è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•

```python
# æ€§èƒ½å›å½’æµ‹è¯•
import pytest

@pytest.mark.performance
@pytest.mark.parametrize("data_size", [1000, 10000, 100000, 1000000])
def test_field_mapping_performance(benchmark, data_size):
    """å­—æ®µæ˜ å°„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    data = generate_test_data(data_size, 10)
    transformer = PolarsFieldMappingTransformer()
    config = {
        "mappings": [
            {"source": "col_0", "target": "category"},
            {"source": "col_1", "target": "value"}
        ]
    }
    
    result = benchmark(transformer.execute, data, config)
    assert len(result.data) == data_size

@pytest.mark.performance
@pytest.mark.parametrize("data_size", [1000, 10000, 100000, 1000000])
def test_aggregation_performance(benchmark, data_size):
    """èšåˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    data = generate_test_data(data_size, 10)
    transformer = DuckDBAggregationTransformer()
    config = {
        "group_by": ["col_0"],
        "aggregations": [
            {"field": "col_1", "function": "sum", "alias": "total"}
        ]
    }
    
    result = benchmark(transformer.execute, data, config)
    assert len(result.data) > 0
```

### 7.2 æ€§èƒ½åŸºå‡†

**è®¾å®šæ€§èƒ½åŸºå‡†ï¼š**

| æ“ä½œ | åŸºå‡†æ—¶é—´ | å†…å­˜ä½¿ç”¨ |
|------|---------|---------|
| å­—æ®µæ˜ å°„ (1Kè¡Œ) | < 0.1s | < 10MB |
| å­—æ®µæ˜ å°„ (100Kè¡Œ) | < 1.0s | < 100MB |
| å­—æ®µæ˜ å°„ (1Mè¡Œ) | < 10s | < 500MB |
| èšåˆ (100Kè¡Œ) | < 2.0s | < 50MB |
| èšåˆ (1Mè¡Œ) | < 10s | < 200MB |
| è½¬æ¢é“¾ (50Kè¡Œ, 3æ­¥) | < 3.0s | < 200MB |

## 8. æ€»ç»“

### 8.1 æ€§èƒ½è¯„çº§

| ç»´åº¦ | è¯„çº§ | è¯´æ˜ |
|------|------|------|
| å•æ“ä½œæ€§èƒ½ | â­â­â­â­â­ | åŸºäº Polars å’Œ DuckDBï¼Œæ€§èƒ½ä¼˜å¼‚ |
| è½¬æ¢é“¾æ€§èƒ½ | â­â­â­â­â­ | å¤šæ­¥éª¤è½¬æ¢æ€§èƒ½æŸè€—å° |
| å†…å­˜æ•ˆç‡ | â­â­â­â­ | å†…å­˜ä½¿ç”¨åˆç†ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ– |
| å¹¶å‘æ€§èƒ½ | â­â­â­ | å­˜åœ¨çº¿ç¨‹å®‰å…¨é—®é¢˜ |
| æ‰©å±•æ€§ | â­â­â­â­â­ | æ‰©å±•æ€§è‰¯å¥½ï¼Œæ¥è¿‘çº¿æ€§ |
| **ç»¼åˆè¯„çº§** | **â­â­â­â­** | **æ•´ä½“æ€§èƒ½ä¼˜ç§€** |

### 8.2 å…³é”®ä¼˜åŒ–ç‚¹

1. **ç«‹å³ä¼˜åŒ–**ï¼š
   - DuckDB è¿æ¥æ± 
   - çº¿ç¨‹å®‰å…¨é—®é¢˜

2. **ä¸­æœŸä¼˜åŒ–**ï¼š
   - è¡¨è¾¾å¼ç¼“å­˜
   - æ‰¹å¤„ç†ä¼˜åŒ–
   - å¹¶è¡Œèšåˆ

3. **é•¿æœŸä¼˜åŒ–**ï¼š
   - JIT ç¼–è¯‘
   - GPU åŠ é€Ÿ
   - åˆ†å¸ƒå¼å¤„ç†

### 8.3 æ€§èƒ½ç›®æ ‡

**ä¼˜åŒ–åçš„æ€§èƒ½ç›®æ ‡ï¼š**

| åœºæ™¯ | å½“å‰æ€§èƒ½ | ç›®æ ‡æ€§èƒ½ | æå‡å¹…åº¦ |
|------|---------|---------|---------|
| å°æ•°æ®é‡ (< 1K) | 0.05s | 0.02s | 60% |
| ä¸­ç­‰æ•°æ®é‡ (100K) | 1.0s | 0.5s | 50% |
| å¤§æ•°æ®é‡ (1M) | 10s | 5s | 50% |
| å¹¶å‘å¤„ç† (10çº¿ç¨‹) | 1.2s | 0.3s | 75% |

é€šè¿‡ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼Œé¢„æœŸæ•´ä½“æ€§èƒ½æå‡ **50-75%**ã€‚
