# QData Transformer åŠŸèƒ½æ‰©å±•å»ºè®®

## 1. æ ¸å¿ƒåŠŸèƒ½æ‰©å±•

### 1.1 æ•°æ®è¿æ¥å™¨ (é«˜ä¼˜å…ˆçº§) ğŸ”µ

**éœ€æ±‚èƒŒæ™¯ï¼š**
å½“å‰åº“åªæ”¯æŒå†…å­˜ä¸­çš„ Polars DataFrame ä½œä¸ºè¾“å…¥ï¼Œé™åˆ¶äº†æ•°æ®æ¥æºçš„å¤šæ ·æ€§ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
# è¿æ¥å™¨åŸºç±»
from abc import ABC, abstractmethod
from typing import Iterator, Any

class DataConnector(ABC):
    """æ•°æ®è¿æ¥å™¨åŸºç±»"""
    
    @abstractmethod
    def read(self, source: str, **kwargs) -> Iterator[pl.DataFrame]:
        """è¯»å–æ•°æ®"""
        pass
    
    @abstractmethod
    def write(self, data: pl.DataFrame, destination: str, **kwargs) -> None:
        """å†™å…¥æ•°æ®"""
        pass

# å…·ä½“è¿æ¥å™¨å®ç°
class CSVConnector(DataConnector):
    """CSV æ–‡ä»¶è¿æ¥å™¨"""
    
    def read(self, source: str, **kwargs) -> Iterator[pl.DataFrame]:
        # æ”¯æŒå¤§æ–‡ä»¶åˆ†å—è¯»å–
        chunk_size = kwargs.get('chunk_size', 10000)
        return pl.read_csv(source, batch_size=chunk_size)

class DatabaseConnector(DataConnector):
    """æ•°æ®åº“è¿æ¥å™¨"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
    
    def read(self, source: str, **kwargs) -> Iterator[pl.DataFrame]:
        # æ”¯æŒ SQL æŸ¥è¯¢
        query = kwargs.get('query', f'SELECT * FROM {source}')
        return pl.read_database(query, self.connection_string)

class S3Connector(DataConnector):
    """S3 è¿æ¥å™¨"""
    
    def __init__(self, bucket: str, access_key: str, secret_key: str):
        self.bucket = bucket
        # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
    
    def read(self, source: str, **kwargs) -> Iterator[pl.DataFrame]:
        # ä» S3 è¯»å–æ•°æ®
        pass

# è¿æ¥å™¨æ³¨å†Œä¸­å¿ƒ
class ConnectorRegistry:
    """è¿æ¥å™¨æ³¨å†Œä¸­å¿ƒ"""
    
    _connectors: dict[str, type[DataConnector]] = {}
    
    @classmethod
    def register(cls, name: str):
        def decorator(connector_class: type[DataConnector]):
            cls._connectors[name] = connector_class
            return connector_class
        return decorator
    
    @classmethod
    def get_connector(cls, name: str, **kwargs) -> DataConnector:
        if name not in cls._connectors:
            raise ValueError(f"è¿æ¥å™¨æœªæ‰¾åˆ°: {name}")
        return cls._connectors[name](**kwargs)
```

### 1.2 è¡¨è¾¾å¼å¼•æ“å¢å¼º (é«˜ä¼˜å…ˆçº§) ğŸ”µ

**éœ€æ±‚èƒŒæ™¯ï¼š**
å½“å‰è¡¨è¾¾å¼è§£æåŠŸèƒ½æœ‰é™ï¼Œä»…æ”¯æŒç®€å•çš„äºŒå…ƒè¿ç®—ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
# å¢å¼ºçš„è¡¨è¾¾å¼å¼•æ“
from lark import Lark, Transformer, v_args

class ExpressionEngine:
    """è¡¨è¾¾å¼å¼•æ“"""
    
    # æ‰©å±•çš„è¯­æ³•æ”¯æŒ
    grammar = r"""
        ?start: sum
        ?sum: product
            | sum "+" product   -> add
            | sum "-" product   -> sub
        ?product: atom
            | product "*" atom  -> mul
            | product "/" atom  -> div
            | product "%" atom  -> mod
        ?atom: NUMBER           -> number
             | CNAME           -> column
             | "(" sum ")"
             | atom "^" atom    -> power
             | "-" atom        -> neg
             | atom "!"        -> factorial
             | FUNCNAME "(" sum ")" -> func
        FUNCNAME: "sin" | "cos" | "tan" | "log" | "abs" | "round"
        %import common.NUMBER
        %import common.CNAME
        %import common.WS_INLINE
        %ignore WS_INLINE
    """
    
    def __init__(self):
        self.parser = Lark(self.grammar, parser='lalr', transformer=self.Transformer())
    
    @v_args(inline=True)
    class Transformer(Transformer):
        from operator import add, sub, mul, truediv as div, mod, pow, neg
        
        def number(self, token):
            return float(token)
        
        def column(self, token):
            return pl.col(str(token))
        
        def factorial(self, n):
            from math import factorial
            return factorial(int(n))
        
        def func(self, name, arg):
            func_map = {
                'sin': lambda x: x.sin(),
                'cos': lambda x: x.cos(),
                'tan': lambda x: x.tan(),
                'log': lambda x: x.log(),
                'abs': lambda x: x.abs(),
                'round': lambda x: x.round(),
            }
            return func_map.get(str(name), lambda x: x)(arg)
    
    def parse(self, expression: str) -> pl.Expr:
        """è§£æè¡¨è¾¾å¼ä¸º Polars è¡¨è¾¾å¼"""
        return self.parser.parse(expression)

# é›†æˆåˆ°è½¬æ¢å™¨
class EnhancedExpressionTransformer(BaseTransformer):
    """å¢å¼ºè¡¨è¾¾å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        self.expression_engine = ExpressionEngine()
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        expressions = []
        for mapping in config.get('mappings', []):
            if mapping.get('transform') == 'expression':
                expr_str = mapping['params']['expr']
                expr = self.expression_engine.parse(expr_str)
                expressions.append(expr.alias(mapping['target']))
        
        return data.with_columns(expressions)
```

### 1.3 ç¼“å­˜æœºåˆ¶ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
é‡å¤è½¬æ¢ç›¸åŒæ•°æ®æ—¶æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æå‡ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
import hashlib
import pickle
from functools import wraps
from typing import Any, Callable

class TransformCache:
    """è½¬æ¢ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_size: int = 1000, ttl: int = 3600):
        self.cache_size = cache_size
        self.ttl = ttl
        self.cache = {}
        self.access_times = {}
    
    def _generate_key(self, data: pl.DataFrame, config: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨æ•°æ®æŒ‡çº¹å’Œé…ç½®å“ˆå¸Œ
        data_hash = hashlib.md5(data.to_pandas().to_numpy().tobytes()).hexdigest()
        config_hash = hashlib.md5(pickle.dumps(config)).hexdigest()
        return f"{data_hash}_{config_hash}"
    
    def get(self, key: str) -> Any:
        """è·å–ç¼“å­˜"""
        if key in self.cache:
            # æ£€æŸ¥ TTL
            import time
            if time.time() - self.access_times[key] < self.ttl:
                return self.cache[key]
            else:
                # è¿‡æœŸï¼Œç§»é™¤ç¼“å­˜
                del self.cache[key]
                del self.access_times[key]
        return None
    
    def put(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜"""
        if len(self.cache) >= self.cache_size:
            # LRU æ¸…ç†
            oldest_key = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_times.clear()

# ç¼“å­˜è£…é¥°å™¨
def with_cache(cache: TransformCache):
    """è½¬æ¢å™¨ç¼“å­˜è£…é¥°å™¨"""
    def decorator(transformer_class: type[BaseTransformer]):
        class CachedTransformer(transformer_class):
            def execute(self, data: pl.DataFrame, config: dict) -> TransformResult:
                cache_key = cache._generate_key(data, config)
                cached_result = cache.get(cache_key)
                
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œè½¬æ¢
                result = super().execute(data, config)
                
                # ç¼“å­˜ç»“æœ
                cache.put(cache_key, result)
                
                return result
        
        return CachedTransformer
    return decorator

# Redis ç¼“å­˜åç«¯
class RedisCacheBackend:
    """Redis ç¼“å­˜åç«¯"""
    
    def __init__(self, redis_url: str):
        import redis
        self.redis_client = redis.from_url(redis_url)
    
    def get(self, key: str) -> Any:
        value = self.redis_client.get(key)
        if value:
            return pickle.loads(value)
        return None
    
    def put(self, key: str, value: Any, ttl: int = 3600) -> None:
        self.redis_client.setex(key, ttl, pickle.dumps(value))
```

### 1.4 æ•°æ®éªŒè¯å’Œæ¨¡å¼æ£€æŸ¥ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
éœ€è¦ç¡®ä¿è¾“å…¥æ•°æ®ç¬¦åˆé¢„æœŸçš„ç»“æ„å’Œç±»å‹ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
from pydantic import BaseModel, validator
from typing import Any, Dict, List, Union

class ColumnSchema(BaseModel):
    """åˆ—æ¨¡å¼å®šä¹‰"""
    name: str
    dtype: str
    nullable: bool = True
    min_value: Union[int, float] = None
    max_value: Union[int, float] = None
    allowed_values: List[str] = None
    
    @validator('dtype')
    def validate_dtype(cls, v):
        allowed_dtypes = {'int64', 'float64', 'string', 'boolean', 'datetime'}
        if v not in allowed_dtypes:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {v}")
        return v

class DataSchema(BaseModel):
    """æ•°æ®æ¨¡å¼å®šä¹‰"""
    columns: List[ColumnSchema]
    min_rows: int = 0
    max_rows: int = None
    
    def validate_dataframe(self, df: pl.DataFrame) -> bool:
        """éªŒè¯ DataFrame"""
        # æ£€æŸ¥åˆ—
        for col_schema in self.columns:
            if col_schema.name not in df.columns:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {col_schema.name}")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            expected_dtype = self._map_dtype(col_schema.dtype)
            actual_dtype = df[col_schema.name].dtype
            if str(actual_dtype) != expected_dtype:
                raise ValueError(f"åˆ— {col_schema.name} ç±»å‹ä¸åŒ¹é…: æœŸæœ› {expected_dtype}, å®é™… {actual_dtype}")
            
            # æ£€æŸ¥å¯ç©ºæ€§
            if not col_schema.nullable and df[col_schema.name].is_null().any():
                raise ValueError(f"åˆ— {col_schema.name} ä¸å…è®¸ç©ºå€¼")
            
            # æ£€æŸ¥å€¼èŒƒå›´
            if col_schema.min_value is not None:
                if (df[col_schema.name] < col_schema.min_value).any():
                    raise ValueError(f"åˆ— {col_schema.name} å€¼å°äºæœ€å°å€¼ {col_schema.min_value}")
            
            if col_schema.max_value is not None:
                if (df[col_schema.name] > col_schema.max_value).any():
                    raise ValueError(f"åˆ— {col_schema.name} å€¼å¤§äºæœ€å¤§å€¼ {col_schema.max_value}")
            
            # æ£€æŸ¥å…è®¸å€¼
            if col_schema.allowed_values is not None:
                invalid_values = set(df[col_schema.name].unique()) - set(col_schema.allowed_values)
                if invalid_values:
                    raise ValueError(f"åˆ— {col_schema.name} åŒ…å«ä¸å…è®¸çš„å€¼: {invalid_values}")
        
        # æ£€æŸ¥è¡Œæ•°
        if len(df) < self.min_rows:
            raise ValueError(f"æ•°æ®è¡Œæ•° {len(df)} å°äºæœ€å°å€¼ {self.min_rows}")
        
        if self.max_rows is not None and len(df) > self.max_rows:
            raise ValueError(f"æ•°æ®è¡Œæ•° {len(df)} å¤§äºæœ€å¤§å€¼ {self.max_rows}")
        
        return True
    
    def _map_dtype(self, dtype: str) -> str:
        """æ˜ å°„æ•°æ®ç±»å‹"""
        dtype_map = {
            'int64': 'Int64',
            'float64': 'Float64',
            'string': 'Utf8',
            'boolean': 'Boolean',
            'datetime': 'Datetime'
        }
        return dtype_map.get(dtype, dtype)

# é›†æˆåˆ°è½¬æ¢å™¨
class SchemaValidatingTransformer(BaseTransformer):
    """æ¨¡å¼éªŒè¯è½¬æ¢å™¨åŒ…è£…å™¨"""
    
    def __init__(self, transformer: BaseTransformer, input_schema: DataSchema = None, output_schema: DataSchema = None):
        self.transformer = transformer
        self.input_schema = input_schema
        self.output_schema = output_schema
    
    def validate_input(self, data: pl.DataFrame) -> None:
        """éªŒè¯è¾“å…¥"""
        if self.input_schema:
            self.input_schema.validate_dataframe(data)
    
    def validate_output(self, data: pl.DataFrame) -> None:
        """éªŒè¯è¾“å‡º"""
        if self.output_schema:
            self.output_schema.validate_dataframe(data)
    
    def execute(self, data: pl.DataFrame, config: dict) -> TransformResult:
        """æ‰§è¡Œå¸¦éªŒè¯çš„è½¬æ¢"""
        # éªŒè¯è¾“å…¥
        self.validate_input(data)
        
        # æ‰§è¡Œè½¬æ¢
        result = self.transformer.execute(data, config)
        
        # éªŒè¯è¾“å‡º
        self.validate_output(result.data)
        
        return result
```

## 2. é«˜çº§åŠŸèƒ½æ‰©å±•

### 2.1 æœºå™¨å­¦ä¹ æ•°æ®é¢„å¤„ç† (é«˜ä¼˜å…ˆçº§) ğŸ”µ

**éœ€æ±‚èƒŒæ™¯ï¼š**
ä¸ºæœºå™¨å­¦ä¹ å·¥ä½œæµæä¾›æ•°æ®é¢„å¤„ç†èƒ½åŠ›ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
import numpy as np

class MLPreprocessorTransformer(BaseTransformer):
    """æœºå™¨å­¦ä¹ é¢„å¤„ç†è½¬æ¢å™¨"""
    
    name = "ml_preprocessor"
    description = "æœºå™¨å­¦ä¹ æ•°æ®é¢„å¤„ç†"
    
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
        self.fitted = False
    
    def fit(self, data: pl.DataFrame, config: dict) -> None:
        """æ‹Ÿåˆé¢„å¤„ç†å‚æ•°"""
        preprocessing_config = config.get('preprocessing', {})
        
        # æ ‡å‡†åŒ–
        for col in preprocessing_config.get('standardize', []):
            scaler = StandardScaler()
            scaler.fit(data[col].to_numpy().reshape(-1, 1))
            self.scalers[col] = scaler
        
        # å½’ä¸€åŒ–
        for col in preprocessing_config.get('normalize', []):
            scaler = MinMaxScaler()
            scaler.fit(data[col].to_numpy().reshape(-1, 1))
            self.scalers[col] = scaler
        
        # æ ‡ç­¾ç¼–ç 
        for col in preprocessing_config.get('label_encode', []):
            encoder = LabelEncoder()
            encoder.fit(data[col].to_numpy())
            self.encoders[col] = encoder
        
        self.fitted = True
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åº”ç”¨é¢„å¤„ç†"""
        if not self.fitted:
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        result = data.clone()
        
        # åº”ç”¨æ ‡å‡†åŒ–/å½’ä¸€åŒ–
        for col, scaler in self.scalers.items():
            if col in result.columns:
                transformed = scaler.transform(result[col].to_numpy().reshape(-1, 1)).flatten()
                result = result.with_columns(pl.Series(transformed).alias(f"{col}_scaled"))
        
        # åº”ç”¨æ ‡ç­¾ç¼–ç 
        for col, encoder in self.encoders.items():
            if col in result.columns:
                encoded = encoder.transform(result[col].to_numpy())
                result = result.with_columns(pl.Series(encoded).alias(f"{col}_encoded"))
        
        return result
    
    def execute(self, data: pl.DataFrame, config: dict) -> TransformResult:
        """æ‰§è¡Œè½¬æ¢ï¼ˆæ”¯æŒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼ï¼‰"""
        mode = config.get('mode', 'transform')
        
        if mode == 'fit':
            self.fit(data, config)
            return TransformResult(data=data, input_rows=len(data), output_rows=len(data))
        else:
            result = self.transform(data, config)
            return TransformResult(data=result, input_rows=len(data), output_rows=len(result))

# ç‰¹å¾å·¥ç¨‹è½¬æ¢å™¨
class FeatureEngineeringTransformer(BaseTransformer):
    """ç‰¹å¾å·¥ç¨‹è½¬æ¢å™¨"""
    
    name = "feature_engineering"
    description = "ç‰¹å¾å·¥ç¨‹è½¬æ¢"
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        features_config = config.get('features', [])
        result = data
        
        for feature_config in features_config:
            feature_type = feature_config['type']
            
            if feature_type == 'polynomial':
                result = self._create_polynomial_features(result, feature_config)
            elif feature_type == 'interaction':
                result = self._create_interaction_features(result, feature_config)
            elif feature_type == 'binning':
                result = self._create_binned_features(result, feature_config)
            elif feature_type == 'datetime':
                result = self._create_datetime_features(result, feature_config)
        
        return result
    
    def _create_polynomial_features(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾"""
        columns = config['columns']
        degree = config.get('degree', 2)
        
        result = data
        for col in columns:
            for d in range(2, degree + 1):
                result = result.with_columns(
                    (pl.col(col) ** d).alias(f"{col}_pow{d}")
                )
        
        return result
    
    def _create_interaction_features(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åˆ›å»ºäº¤äº’ç‰¹å¾"""
        interactions = config['interactions']
        
        result = data
        for interaction in interactions:
            col1, col2 = interaction['columns']
            result = result.with_columns(
                (pl.col(col1) * pl.col(col2)).alias(f"{col1}_{col2}_interaction")
            )
        
        return result
    
    def _create_binned_features(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åˆ›å»ºåˆ†ç®±ç‰¹å¾"""
        column = config['column']
        bins = config['bins']
        labels = config.get('labels')
        
        result = data
        binned = pl.cut(pl.col(column), bins=bins, labels=labels)
        result = result.with_columns(binned.alias(f"{column}_binned"))
        
        return result
    
    def _create_datetime_features(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åˆ›å»ºæ—¥æœŸæ—¶é—´ç‰¹å¾"""
        column = config['column']
        features = config.get('features', ['year', 'month', 'day', 'weekday'])
        
        result = data
        col = pl.col(column)
        
        feature_map = {
            'year': col.dt.year(),
            'month': col.dt.month(),
            'day': col.dt.day(),
            'weekday': col.dt.weekday(),
            'hour': col.dt.hour(),
            'minute': col.dt.minute(),
            'quarter': col.dt.quarter(),
            'is_weekend': col.dt.weekday() >= 5
        }
        
        for feature in features:
            if feature in feature_map:
                result = result.with_columns(
                    feature_map[feature].alias(f"{column}_{feature}")
                )
        
        return result
```

### 2.2 æ•°æ®è´¨é‡ç›‘æ§ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
åœ¨æ•°æ®è½¬æ¢è¿‡ç¨‹ä¸­ç›‘æ§æ•°æ®è´¨é‡ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
from dataclasses import dataclass
from typing import Dict, List, Any

@dataclass
class DataQualityMetric:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    metric_name: str
    column: str
    value: float
    threshold: float = None
    status: str = "PASS"  # PASS, WARNING, FAIL

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: List[DataQualityMetric] = []
    
    def check_completeness(self, data: pl.DataFrame, column: str) -> DataQualityMetric:
        """æ£€æŸ¥å®Œæ•´æ€§ï¼ˆç©ºå€¼ç‡ï¼‰"""
        null_count = data[column].is_null().sum()
        total_count = len(data)
        null_rate = null_count / total_count
        
        return DataQualityMetric(
            metric_name="completeness",
            column=column,
            value=1 - null_rate,
            threshold=0.95,
            status="FAIL" if null_rate > 0.05 else "PASS"
        )
    
    def check_uniqueness(self, data: pl.DataFrame, column: str) -> DataQualityMetric:
        """æ£€æŸ¥å”¯ä¸€æ€§"""
        unique_count = data[column].n_unique()
        total_count = len(data)
        uniqueness = unique_count / total_count
        
        return DataQualityMetric(
            metric_name="uniqueness",
            column=column,
            value=uniqueness,
            threshold=0.9,
            status="FAIL" if uniqueness < 0.9 else "PASS"
        )
    
    def check_validity(self, data: pl.DataFrame, column: str, validation_func) -> DataQualityMetric:
        """æ£€æŸ¥æœ‰æ•ˆæ€§"""
        valid_count = validation_func(data[column]).sum()
        total_count = len(data)
        validity = valid_count / total_count
        
        return DataQualityMetric(
            metric_name="validity",
            column=column,
            value=validity,
            threshold=0.99,
            status="FAIL" if validity < 0.99 else "PASS"
        )
    
    def check_consistency(self, data: pl.DataFrame, column1: str, column2: str) -> DataQualityMetric:
        """æ£€æŸ¥ä¸€è‡´æ€§"""
        # ç¤ºä¾‹ï¼šæ£€æŸ¥ä¸¤åˆ—çš„ç›¸å…³æ€§
        correlation = data[column1].corr(data[column2])
        
        return DataQualityMetric(
            metric_name="consistency",
            column=f"{column1}_{column2}",
            value=abs(correlation),
            threshold=0.7,
            status="WARNING" if abs(correlation) < 0.7 else "PASS"
        )
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        total_metrics = len(self.metrics)
        passed_metrics = sum(1 for m in self.metrics if m.status == "PASS")
        failed_metrics = sum(1 for m in self.metrics if m.status == "FAIL")
        warning_metrics = sum(1 for m in self.metrics if m.status == "WARNING")
        
        return {
            "summary": {
                "total": total_metrics,
                "passed": passed_metrics,
                "failed": failed_metrics,
                "warning": warning_metrics,
                "score": passed_metrics / total_metrics if total_metrics > 0 else 1.0
            },
            "details": [
                {
                    "metric": m.metric_name,
                    "column": m.column,
                    "value": m.value,
                    "threshold": m.threshold,
                    "status": m.status
                }
                for m in self.metrics
            ]
        }

# é›†æˆåˆ°è½¬æ¢å™¨
class DataQualityTransformer(BaseTransformer):
    """æ•°æ®è´¨é‡æ£€æŸ¥è½¬æ¢å™¨"""
    
    name = "data_quality_check"
    description = "æ•°æ®è´¨é‡æ£€æŸ¥"
    
    def __init__(self):
        self.monitor = DataQualityMonitor()
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        checks = config.get('checks', [])
        
        for check in checks:
            check_type = check['type']
            column = check['column']
            
            if check_type == 'completeness':
                metric = self.monitor.check_completeness(data, column)
                self.monitor.metrics.append(metric)
            elif check_type == 'uniqueness':
                metric = self.monitor.check_uniqueness(data, column)
                self.monitor.metrics.append(metric)
            elif check_type == 'validity':
                validation_func = check['validation_func']
                metric = self.monitor.check_validity(data, column, validation_func)
                self.monitor.metrics.append(metric)
            elif check_type == 'consistency':
                column2 = check['other_column']
                metric = self.monitor.check_consistency(data, column, column2)
                self.monitor.metrics.append(metric)
        
        # æ•°æ®è´¨é‡æŠ¥å‘Šå¯ä»¥ä½œä¸ºå…ƒæ•°æ®è¿”å›
        report = self.monitor.generate_report()
        print(f"æ•°æ®è´¨é‡æŠ¥å‘Š: {report}")
        
        return data
    
    def execute(self, data: pl.DataFrame, config: dict) -> TransformResult:
        result = self.transform(data, config)
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        report = self.monitor.generate_report()
        
        return TransformResult(
            data=result,
            input_rows=len(data),
            output_rows=len(result),
            metadata={
                "quality_report": report,
                "quality_score": report["summary"]["score"]
            }
        )
```

### 2.3 æµå¼æ•°æ®å¤„ç† (ä½ä¼˜å…ˆçº§) ğŸŸ¢

**éœ€æ±‚èƒŒæ™¯ï¼š**
å¤„ç†è¶…å¤§æ•°æ®é›†æˆ–å®æ—¶æ•°æ®æµã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
from typing import Iterator, Optional
import asyncio

class StreamTransformer:
    """æµå¼æ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, transformers: List[BaseTransformer], buffer_size: int = 1000):
        self.transformers = transformers
        self.buffer_size = buffer_size
    
    def process_stream(self, stream: Iterator[pl.DataFrame], config: dict) -> Iterator[pl.DataFrame]:
        """å¤„ç†æ•°æ®æµ"""
        buffer = []
        
        for chunk in stream:
            buffer.append(chunk)
            
            if len(buffer) >= self.buffer_size:
                # åˆå¹¶ç¼“å†²åŒºæ•°æ®
                combined = pl.concat(buffer)
                
                # åº”ç”¨è½¬æ¢
                for transformer in self.transformers:
                    combined = transformer.execute(combined, config).data
                
                yield combined
                buffer = []
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if buffer:
            combined = pl.concat(buffer)
            for transformer in self.transformers:
                combined = transformer.execute(combined, config).data
            yield combined

# å¼‚æ­¥æµå¤„ç†
class AsyncStreamTransformer:
    """å¼‚æ­¥æµå¼æ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, transformers: List[BaseTransformer], max_concurrent: int = 4):
        self.transformers = transformers
        self.max_concurrent = max_concurrent
        self.queue = asyncio.Queue(maxsize=100)
    
    async def process_stream_async(self, stream: Iterator[pl.DataFrame], config: dict) -> Iterator[pl.DataFrame]:
        """å¼‚æ­¥å¤„ç†æ•°æ®æµ"""
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        tasks = []
        
        for chunk in stream:
            task = asyncio.create_task(self._process_chunk(chunk, config))
            tasks.append(task)
            
            if len(tasks) >= self.max_concurrent:
                # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                for task in done:
                    yield await task
                tasks = list(pending)
        
        # å¤„ç†å‰©ä½™ä»»åŠ¡
        if tasks:
            done, _ = await asyncio.wait(tasks)
            for task in done:
                yield await task
    
    async def _process_chunk(self, chunk: pl.DataFrame, config: dict) -> pl.DataFrame:
        """å¤„ç†æ•°æ®å—"""
        result = chunk
        for transformer in self.transformers:
            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­æ‰§è¡Œè½¬æ¢
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                lambda: transformer.execute(result, config).data
            )
        return result
```

## 3. å·¥å…·å’Œè¾…åŠ©åŠŸèƒ½

### 3.1 å¯è§†åŒ–å·¥å…· (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
å¸®åŠ©ç”¨æˆ·ç†è§£å’Œè°ƒè¯•æ•°æ®è½¬æ¢è¿‡ç¨‹ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
import matplotlib.pyplot as plt
import seaborn as sns
from graphviz import Digraph

class TransformVisualizer:
    """è½¬æ¢å¯è§†åŒ–å·¥å…·"""
    
    def visualize_chain(self, chain: TransformChain) -> Digraph:
        """å¯è§†åŒ–è½¬æ¢é“¾"""
        dot = Digraph(comment='Transform Chain')
        
        # æ·»åŠ èŠ‚ç‚¹
        for i, step in enumerate(chain.steps):
            node_id = f"step_{i}"
            label = f"{step.name or step.transformer_name}\n{step.transformer_name}"
            dot.node(node_id, label, shape='box')
            
            # æ·»åŠ è¾¹
            if i > 0:
                dot.edge(f"step_{i-1}", node_id)
        
        return dot
    
    def visualize_data_profile(self, data: pl.DataFrame, output_path: str) -> None:
        """å¯è§†åŒ–æ•°æ®æ¦‚å†µ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ•°å€¼åˆ—åˆ†å¸ƒ
        numeric_cols = [col for col in data.columns if data[col].dtype in (pl.Int64, pl.Float64)]
        if numeric_cols:
            data[numeric_cols].describe().to_pandas().plot(kind='bar', ax=axes[0, 0])
            axes[0, 0].set_title('Numeric Columns Statistics')
        
        # ç¼ºå¤±å€¼çƒ­å›¾
        missing_data = data.select([pl.col(col).is_null().sum() for col in data.columns])
        if missing_data.shape[1] > 0:
            sns.heatmap(missing_data.to_pandas(), annot=True, ax=axes[0, 1])
            axes[0, 1].set_title('Missing Values')
        
        # ç›¸å…³æ€§çŸ©é˜µ
        if len(numeric_cols) > 1:
            corr_matrix = data[numeric_cols].corr().to_pandas()
            sns.heatmap(corr_matrix, annot=True, ax=axes[1, 0])
            axes[1, 0].set_title('Correlation Matrix')
        
        # æ•°æ®ç±»å‹åˆ†å¸ƒ
        dtype_counts = data.dtypes.value_counts()
        axes[1, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')
        axes[1, 1].set_title('Data Types Distribution')
        
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()
    
    def visualize_transformation_impact(self, before: pl.DataFrame, after: pl.DataFrame, output_path: str) -> None:
        """å¯è§†åŒ–è½¬æ¢å½±å“"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # è¡Œæ•°å˜åŒ–
        axes[0].bar(['Before', 'After'], [len(before), len(after)])
        axes[0].set_title('Row Count')
        axes[0].set_ylabel('Number of Rows')
        
        # åˆ—æ•°å˜åŒ–
        axes[1].bar(['Before', 'After'], [len(before.columns), len(after.columns)])
        axes[1].set_title('Column Count')
        axes[1].set_ylabel('Number of Columns')
        
        # å†…å­˜ä½¿ç”¨å˜åŒ–
        before_memory = before.estimated_size()
        after_memory = after.estimated_size()
        axes[2].bar(['Before', 'After'], [before_memory, after_memory])
        axes[2].set_title('Memory Usage')
        axes[2].set_ylabel('Bytes')
        
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()
```

### 3.2 é…ç½®ç”Ÿæˆå™¨ (ä½ä¼˜å…ˆçº§) ğŸŸ¢

**éœ€æ±‚èƒŒæ™¯ï¼š**
å¸®åŠ©ç”¨æˆ·ç”Ÿæˆè½¬æ¢é…ç½®ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
class ConfigGenerator:
    """é…ç½®ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_field_mapping_config(data: pl.DataFrame, mapping_type: str = 'direct') -> dict:
        """ç”Ÿæˆå­—æ®µæ˜ å°„é…ç½®"""
        mappings = []
        
        for col in data.columns:
            if mapping_type == 'direct':
                mappings.append({
                    "source": col,
                    "target": col
                })
            elif mapping_type == 'uppercase':
                mappings.append({
                    "source": col,
                    "target": col.upper()
                })
            elif mapping_type == 'snake_case':
                import re
                snake_case = re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower()
                mappings.append({
                    "source": col,
                    "target": snake_case
                })
        
        return {"mappings": mappings}
    
    @staticmethod
    def generate_aggregation_config(group_by: List[str], agg_columns: List[str]) -> dict:
        """ç”Ÿæˆèšåˆé…ç½®"""
        aggregations = []
        
        for col in agg_columns:
            aggregations.extend([
                {"field": col, "function": "sum", "alias": f"{col}_sum"},
                {"field": col, "function": "avg", "alias": f"{col}_avg"},
                {"field": col, "function": "min", "alias": f"{col}_min"},
                {"field": col, "function": "max", "alias": f"{col}_max"}
            ])
        
        return {
            "group_by": group_by,
            "aggregations": aggregations
        }
    
    @staticmethod
    def generate_ml_preprocessing_config(data: pl.DataFrame, target_column: str = None) -> dict:
        """ç”Ÿæˆæœºå™¨å­¦ä¹ é¢„å¤„ç†é…ç½®"""
        config = {
            "preprocessing": {
                "standardize": [],
                "normalize": [],
                "label_encode": [],
                "one_hot_encode": []
            }
        }
        
        for col in data.columns:
            if col == target_column:
                continue
                
            dtype = data[col].dtype
            
            if dtype in (pl.Int64, pl.Float64):
                config["preprocessing"]["standardize"].append(col)
            elif dtype == pl.Utf8:
                unique_ratio = data[col].n_unique() / len(data)
                if unique_ratio < 0.1:  # ä½åŸºæ•°åˆ†ç±»å˜é‡
                    config["preprocessing"]["one_hot_encode"].append(col)
                else:  # é«˜åŸºæ•°åˆ†ç±»å˜é‡
                    config["preprocessing"]["label_encode"].append(col)
        
        return config
```

## 4. æ€§èƒ½ä¼˜åŒ–æ‰©å±•

### 4.1 å¹¶è¡Œå¤„ç†æ¡†æ¶ (é«˜ä¼˜å…ˆçº§) ğŸ”µ

**éœ€æ±‚èƒŒæ™¯ï¼š**
å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU æå‡å¤„ç†æ€§èƒ½ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import Manager
import multiprocessing as mp

class ParallelTransformer(BaseTransformer):
    """å¹¶è¡Œè½¬æ¢å™¨"""
    
    name = "parallel_processor"
    description = "å¹¶è¡Œæ•°æ®å¤„ç†"
    
    def __init__(self, n_workers: int = None, strategy: str = 'process'):
        self.n_workers = n_workers or mp.cpu_count()
        self.strategy = strategy  # 'process' æˆ– 'thread'
        self.executor_class = ProcessPoolExecutor if strategy == 'process' else ThreadPoolExecutor
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """å¹¶è¡Œè½¬æ¢æ•°æ®"""
        # æ•°æ®åˆ†åŒº
        partitions = self._partition_data(data, self.n_workers)
        
        # è·å–å­è½¬æ¢å™¨é…ç½®
        sub_transformer_config = config.get('sub_transformer', {})
        transformer_name = sub_transformer_config.get('name')
        transformer_config = sub_transformer_config.get('config', {})
        
        if not transformer_name:
            raise ValueError("å¿…é¡»æŒ‡å®šå­è½¬æ¢å™¨åç§°")
        
        # å¹¶è¡Œå¤„ç†
        with self.executor_class(max_workers=self.n_workers) as executor:
            futures = [
                executor.submit(
                    self._process_partition,
                    partition,
                    transformer_name,
                    transformer_config
                )
                for partition in partitions
            ]
            
            results = [future.result() for future in futures]
        
        # åˆå¹¶ç»“æœ
        return pl.concat(results)
    
    def _partition_data(self, data: pl.DataFrame, n_partitions: int) -> List[pl.DataFrame]:
        """æ•°æ®åˆ†åŒº"""
        chunk_size = len(data) // n_partitions
        partitions = []
        
        for i in range(n_partitions):
            start = i * chunk_size
            end = start + chunk_size if i < n_partitions - 1 else len(data)
            partitions.append(data[start:end])
        
        return partitions
    
    def _process_partition(self, partition: pl.DataFrame, transformer_name: str, config: dict) -> pl.DataFrame:
        """å¤„ç†æ•°æ®åˆ†åŒº"""
        # åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­é‡æ–°è·å–è½¬æ¢å™¨å®ä¾‹
        transformer = TransformerRegistry.get(transformer_name)
        result = transformer.execute(partition, config)
        return result.data

# åˆ†å¸ƒå¼å¤„ç†
class DistributedTransformer(BaseTransformer):
    """åˆ†å¸ƒå¼è½¬æ¢å™¨"""
    
    name = "distributed_processor"
    description = "åˆ†å¸ƒå¼æ•°æ®å¤„ç†"
    
    def __init__(self, cluster_address: str):
        self.cluster_address = cluster_address
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è®¡ç®—å®¢æˆ·ç«¯ï¼ˆå¦‚ Daskã€Rayï¼‰
    
    def transform(self, data: pl.DataFrame, config: dict) -> pl.DataFrame:
        """åˆ†å¸ƒå¼è½¬æ¢"""
        # å°†æ•°æ®åˆ†å‘åˆ°é›†ç¾¤
        # åœ¨é›†ç¾¤ä¸Šæ‰§è¡Œè½¬æ¢
        # æ”¶é›†ç»“æœ
        pass
```

### 4.2 ç¼“å­˜ç³»ç»Ÿ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
é¿å…é‡å¤è®¡ç®—ï¼Œæå‡æ€§èƒ½ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

å·²åœ¨æ ¸å¿ƒåŠŸèƒ½æ‰©å±•ä¸­è¯¦ç»†æè¿°ï¼Œæ­¤å¤„è¡¥å……ç¼“å­˜ç­–ç•¥ï¼š

```python
class CacheStrategy:
    """ç¼“å­˜ç­–ç•¥åŸºç±»"""
    
    @abstractmethod
    def should_cache(self, data_size: int, computation_cost: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”ç¼“å­˜"""
        pass

class SizeBasedCacheStrategy(CacheStrategy):
    """åŸºäºæ•°æ®å¤§å°çš„ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self, max_data_size: int = 1000000):
        self.max_data_size = max_data_size
    
    def should_cache(self, data_size: int, computation_cost: float) -> bool:
        return data_size <= self.max_data_size

class CostBasedCacheStrategy(CacheStrategy):
    """åŸºäºè®¡ç®—æˆæœ¬çš„ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self, min_computation_cost: float = 1.0):
        self.min_computation_cost = min_computation_cost
    
    def should_cache(self, data_size: int, computation_cost: float) -> bool:
        return computation_cost >= self.min_computation_cost

class AdaptiveCacheStrategy(CacheStrategy):
    """è‡ªé€‚åº”ç¼“å­˜ç­–ç•¥"""
    
    def should_cache(self, data_size: int, computation_cost: float) -> bool:
        # ç¼“å­˜æ•ˆç›Š = è®¡ç®—æˆæœ¬ / æ•°æ®å¤§å°
        benefit = computation_cost / (data_size + 1)
        return benefit > 0.001  # é˜ˆå€¼å¯é…ç½®
```

## 5. ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### 5.1 æ€§èƒ½ç›‘æ§ (é«˜ä¼˜å…ˆçº§) ğŸ”µ

**éœ€æ±‚èƒŒæ™¯ï¼š**
ç›‘æ§è½¬æ¢æ€§èƒ½ï¼Œå‘ç°æ€§èƒ½ç“¶é¢ˆã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
import time
import psutil
from dataclasses import dataclass, field
from typing import Dict, List, Any
import json

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    execution_time: float
    cpu_usage: float
    memory_peak: float
    memory_avg: float
    io_read_bytes: int
    io_write_bytes: int
    throughput: float  # rows/second
    
class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.process = psutil.Process()
    
    def profile(self, func: Callable) -> Callable:
        """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            # å¼€å§‹ç›‘æ§
            start_time = time.time()
            start_memory = self.process.memory_info().rss
            start_io = self.process.io_counters()
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # ç»“æŸç›‘æ§
            end_time = time.time()
            end_memory = self.process.memory_info().rss
            end_io = self.process.io_counters()
            
            # è®¡ç®—æŒ‡æ ‡
            execution_time = end_time - start_time
            data_size = len(args[0]) if args else 0  # å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ•°æ®
            throughput = data_size / execution_time if execution_time > 0 else 0
            
            metrics = PerformanceMetrics(
                execution_time=execution_time,
                cpu_usage=self.process.cpu_percent(),
                memory_peak=max(start_memory, end_memory),
                memory_avg=(start_memory + end_memory) / 2,
                io_read_bytes=end_io.read_bytes - start_io.read_bytes,
                io_write_bytes=end_io.write_bytes - start_io.write_bytes,
                throughput=throughput
            )
            
            self.metrics_history.append(metrics)
            
            return result
        
        return wrapper
    
    def get_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics_history:
            return {}
        
        avg_execution_time = sum(m.execution_time for m in self.metrics_history) / len(self.metrics_history)
        avg_throughput = sum(m.throughput for m in self.metrics_history) / len(self.metrics_history)
        peak_memory = max(m.memory_peak for m in self.metrics_history)
        
        return {
            "summary": {
                "total_operations": len(self.metrics_history),
                "avg_execution_time": avg_execution_time,
                "avg_throughput": avg_throughput,
                "peak_memory_mb": peak_memory / 1024 / 1024
            },
            "details": [
                {
                    "execution_time": m.execution_time,
                    "throughput": m.throughput,
                    "memory_peak_mb": m.memory_peak / 1024 / 1024,
                    "cpu_usage": m.cpu_usage
                }
                for m in self.metrics_history
            ]
        }

# é›†æˆ Prometheus
from prometheus_client import Counter, Histogram, Gauge, start_http_server

class PrometheusMonitor:
    """Prometheus ç›‘æ§é›†æˆ"""
    
    def __init__(self, port: int = 8000):
        self.port = port
        
        # æŒ‡æ ‡å®šä¹‰
        self.execution_counter = Counter(
            'transformer_executions_total',
            'Total number of transformer executions',
            ['transformer_name', 'status']
        )
        
        self.execution_duration = Histogram(
            'transformer_execution_duration_seconds',
            'Execution duration in seconds',
            ['transformer_name']
        )
        
        self.memory_usage = Gauge(
            'transformer_memory_usage_bytes',
            'Memory usage in bytes',
            ['transformer_name']
        )
        
        self.throughput_gauge = Gauge(
            'transformer_throughput_rows_per_second',
            'Processing throughput in rows per second',
            ['transformer_name']
        )
        
        # å¯åŠ¨ HTTP æœåŠ¡å™¨
        start_http_server(port)
    
    def record_execution(self, transformer_name: str, duration: float, status: str, data_size: int):
        """è®°å½•æ‰§è¡ŒæŒ‡æ ‡"""
        self.execution_counter.labels(transformer_name=transformer_name, status=status).inc()
        self.execution_duration.labels(transformer_name=transformer_name).observe(duration)
        
        throughput = data_size / duration if duration > 0 else 0
        self.throughput_gauge.labels(transformer_name=transformer_name).set(throughput)
    
    def record_memory(self, transformer_name: str, memory_bytes: int):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        self.memory_usage.labels(transformer_name=transformer_name).set(memory_bytes)
```

### 5.2 æ—¥å¿—ç³»ç»Ÿ (ä¸­ä¼˜å…ˆçº§) ğŸŸ¡

**éœ€æ±‚èƒŒæ™¯ï¼š**
è¯¦ç»†çš„æ—¥å¿—è®°å½•ä¾¿äºè°ƒè¯•å’Œé—®é¢˜æ’æŸ¥ã€‚

**åŠŸèƒ½è®¾è®¡ï¼š**

```python
import logging
import json
from typing import Any, Dict
from datetime import datetime

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name: str, level: str = 'INFO'):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # é…ç½®å¤„ç†å™¨
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_transform_start(self, transformer_name: str, data_size: int, config: Dict[str, Any]):
        """è®°å½•è½¬æ¢å¼€å§‹"""
        self.logger.info(
            "Transform started",
            extra={
                "event": "transform_start",
                "transformer": transformer_name,
                "data_size": data_size,
                "config": config
            }
        )
    
    def log_transform_end(self, transformer_name: str, duration: float, status: str, error: str = None):
        """è®°å½•è½¬æ¢ç»“æŸ"""
        extra = {
            "event": "transform_end",
            "transformer": transformer_name,
            "duration": duration,
            "status": status
        }
        
        if error:
            extra["error"] = error
            self.logger.error("Transform failed", extra=extra)
        else:
            self.logger.info("Transform completed", extra=extra)
    
    def log_data_quality_issue(self, issue_type: str, column: str, details: Dict[str, Any]):
        """è®°å½•æ•°æ®è´¨é‡é—®é¢˜"""
        self.logger.warning(
            "Data quality issue detected",
            extra={
                "event": "data_quality_issue",
                "type": issue_type,
                "column": column,
                "details": details
            }
        )
    
    def log_performance_warning(self, transformer_name: str, metric: str, value: float, threshold: float):
        """è®°å½•æ€§èƒ½è­¦å‘Š"""
        self.logger.warning(
            "Performance issue detected",
            extra={
                "event": "performance_warning",
                "transformer": transformer_name,
                "metric": metric,
                "value": value,
                "threshold": threshold
            }
        )

# JSON æ ¼å¼çš„æ—¥å¿—å¤„ç†å™¨
class JSONLogHandler(logging.Handler):
    """JSON æ ¼å¼æ—¥å¿—å¤„ç†å™¨"""
    
    def __init__(self, filename: str):
        super().__init__()
        self.filename = filename
    
    def emit(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "line": record.lineno
        }
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        if hasattr(record, 'extra'):
            log_entry.update(record.extra)
        
        # å†™å…¥æ–‡ä»¶
        with open(self.filename, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
```

## 6. æ€»ç»“

### 6.1 åŠŸèƒ½æ‰©å±•ä¼˜å…ˆçº§

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | é¢„è®¡å·¥ä½œé‡ | ä»·å€¼ |
|------|--------|-----------|------|
| æ•°æ®è¿æ¥å™¨ | ğŸ”µ é«˜ | 2-3 å‘¨ | â­â­â­â­â­ |
| è¡¨è¾¾å¼å¼•æ“å¢å¼º | ğŸ”µ é«˜ | 1-2 å‘¨ | â­â­â­â­â­ |
| æœºå™¨å­¦ä¹ é¢„å¤„ç† | ğŸ”µ é«˜ | 2-3 å‘¨ | â­â­â­â­ |
| ç¼“å­˜æœºåˆ¶ | ğŸŸ¡ ä¸­ | 1-2 å‘¨ | â­â­â­â­ |
| æ•°æ®è´¨é‡ç›‘æ§ | ğŸŸ¡ ä¸­ | 2-3 å‘¨ | â­â­â­â­ |
| å¹¶è¡Œå¤„ç†æ¡†æ¶ | ğŸŸ¡ ä¸­ | 3-4 å‘¨ | â­â­â­â­ |
| å¯è§†åŒ–å·¥å…· | ğŸŸ¡ ä¸­ | 2-3 å‘¨ | â­â­â­ |
| æ€§èƒ½ç›‘æ§ | ğŸ”µ é«˜ | 1-2 å‘¨ | â­â­â­â­ |
| æµå¼å¤„ç† | ğŸŸ¢ ä½ | 4-5 å‘¨ | â­â­â­ |

### 6.2 å®æ–½å»ºè®®

**ç¬¬ä¸€é˜¶æ®µ (1-2 ä¸ªæœˆ)ï¼š**
1. å®ç°æ•°æ®è¿æ¥å™¨ï¼ˆCSVã€æ•°æ®åº“ï¼‰
2. å¢å¼ºè¡¨è¾¾å¼å¼•æ“
3. æ·»åŠ åŸºç¡€æ€§èƒ½ç›‘æ§
4. å®ç°ç¼“å­˜æœºåˆ¶

**ç¬¬äºŒé˜¶æ®µ (2-3 ä¸ªæœˆ)ï¼š**
1. æ·»åŠ æœºå™¨å­¦ä¹ é¢„å¤„ç†åŠŸèƒ½
2. å®ç°æ•°æ®è´¨é‡ç›‘æ§
3. æ·»åŠ å¹¶è¡Œå¤„ç†æ¡†æ¶
4. å®Œå–„å¯è§†åŒ–å·¥å…·

**ç¬¬ä¸‰é˜¶æ®µ (3-6 ä¸ªæœˆ)ï¼š**
1. æ·»åŠ æ›´å¤šè¿æ¥å™¨ï¼ˆS3ã€API ç­‰ï¼‰
2. å®ç°æµå¼æ•°æ®å¤„ç†
3. æ·»åŠ é«˜çº§ç›‘æ§å’Œå‘Šè­¦
4. æŒç»­ä¼˜åŒ–æ€§èƒ½

### 6.3 é¢„æœŸæ•ˆæœ

é€šè¿‡åŠŸèƒ½æ‰©å±•ï¼ŒQData Transformer å°†ä»å•ä¸€çš„æ•°æ®è½¬æ¢åº“æ¼”å˜ä¸ºï¼š

1. **å®Œæ•´çš„æ•°æ®å¤„ç†å¹³å°** - æ”¯æŒå¤šç§æ•°æ®æºå’Œç›®æ ‡
2. **é«˜æ€§èƒ½è®¡ç®—å¼•æ“** - æ”¯æŒå¹¶è¡Œå’Œåˆ†å¸ƒå¼å¤„ç†
3. **æ™ºèƒ½æ•°æ®ç®¡é“** - é›†æˆæœºå™¨å­¦ä¹ å’Œæ•°æ®è´¨é‡ç›‘æ§
4. **å¯è§‚æµ‹ç³»ç»Ÿ** - å®Œå–„çš„ç›‘æ§ã€æ—¥å¿—å’Œå¯è§†åŒ–èƒ½åŠ›

è¿™å°†å¤§å¤§æå‡åº“çš„ç«äº‰åŠ›å’Œå®ç”¨æ€§ï¼Œä½¿å…¶æˆä¸ºæ•°æ®å·¥ç¨‹é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚
